{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93283f37-9e18-4453-859a-d527872d1db3",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning  \n",
    "\n",
    "## What is Deep Learning?  \n",
    "Deep Learning is a **subset of Machine Learning (ML)** that uses **neural networks** with multiple layers to process data.  \n",
    "It is designed to **mimic the human brain** and is used in tasks like image recognition, speech processing, and decision-making.\n",
    "\n",
    "## Why Use Deep Learning?  \n",
    "- Can handle **complex patterns** in large datasets.  \n",
    "- Works well for **image, text, and speech-based applications**.  \n",
    "- Learns features **automatically** without manual selection.\n",
    "\n",
    "## Key Deep Learning Algorithms  \n",
    "\n",
    "### 1. Artificial Neural Network (ANN)  \n",
    "A neural network that consists of multiple layers of **neurons**:  \n",
    "- **Input Layer:** Takes raw data.  \n",
    "- **Hidden Layers:** Process the information.  \n",
    "- **Output Layer:** Provides the final result.  \n",
    "\n",
    "ðŸ”¹ **Common ANN Algorithms:**  \n",
    "- **Multilayer Perceptron (MLP):** Fully connected network used for classification and regression.  \n",
    "- **Backpropagation:** Optimizes the network by adjusting weights.  \n",
    "\n",
    "### 2. Convolutional Neural Network (CNN)  \n",
    "A type of neural network designed for **image and video processing**.  \n",
    "- Uses **filters (kernels)** to detect edges, colors, and patterns.  \n",
    "- Works well for **object detection, facial recognition, and medical imaging**.  \n",
    "\n",
    "ðŸ”¹ **Common CNN Architectures:**  \n",
    "- **LeNet-5:** Early CNN used for handwritten digit recognition.  \n",
    "- **AlexNet:** Deeper network used in ImageNet competition.  \n",
    "- **VGGNet:** Uses multiple small filters to improve accuracy.  \n",
    "- **ResNet:** Introduces skip connections to avoid vanishing gradients.  \n",
    "\n",
    "### 3. Recurrent Neural Network (RNN)  \n",
    "A neural network designed for **sequential data** like text and time series.  \n",
    "- Uses **feedback connections** to remember previous inputs.  \n",
    "- Works well for **language modeling, speech recognition, and time-series forecasting**.  \n",
    "\n",
    "ðŸ”¹ **Common RNN Variants:**  \n",
    "- **Simple RNN:** Basic version but suffers from vanishing gradients.  \n",
    "- **Long Short-Term Memory (LSTM):** Handles long-term dependencies.  \n",
    "- **Gated Recurrent Unit (GRU):** Similar to LSTM but computationally efficient.  \n",
    "- **Bidirectional RNN:** Processes data in both forward and backward directions.  \n",
    "\n",
    "## Applications of Deep Learning  \n",
    "- **Computer Vision:** Object detection, facial recognition.  \n",
    "- **Natural Language Processing (NLP):** Chatbots, language translation.  \n",
    "- **Speech Recognition:** Voice assistants like Siri and Alexa.  \n",
    "- **Autonomous Vehicles:** Self-driving cars use deep learning for object detection.  \n",
    "\n",
    "---\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628969c8-e196-411a-8381-9fa133af6529",
   "metadata": {},
   "source": [
    "# Why is Deep Learning Becoming Popular?  \n",
    "\n",
    "### The Problem in 2011  \n",
    "By 2011, companies like **Google** and **Facebook** were collecting vast amounts of data, but faced two key challenges:  \n",
    "1. **Limited computational power** â€“ Traditional CPUs couldn't process data efficiently.  \n",
    "2. **Traditional machine learning limitations** â€“ Models struggled with complex data like images and speech.\n",
    "\n",
    "### Key Breakthroughs in 2012  \n",
    "In **2012**, **Geoffrey Hinton**, **Alex Krizhevsky**, and **Ilya Sutskever** developed **AlexNet**, a deep Convolutional Neural Network (CNN) that outperformed previous models in the **ImageNet competition**. This demonstrated the power of deep learning, especially when paired with **large data sets** and **GPUs**. \n",
    "\n",
    "### Deep Learning Gains Momentum (2015 Onwards)  \n",
    "By 2015, companies realized the value of deep learning for business applications. Key developments included:  \n",
    "- **Faster hardware** (GPUs and later TPUs) made training deep models more efficient.  \n",
    "- **Improved algorithms**, such as **ResNet** for deep networks and **LSTMs** for sequential data, further enhanced model performance.  \n",
    "- More **data** allowed models to perform better and scale effectively.\n",
    "\n",
    "### Real-World Applications  \n",
    "- **Google Translate** â€“ Uses deep learning for real-time text translation.  \n",
    "- **Teslaâ€™s Autopilot** â€“ Deep learning enables self-driving cars to detect objects and navigate.  \n",
    "- **AlphaGo** â€“ DeepMindâ€™s AI defeated the world champion at the complex game of Go.\n",
    "\n",
    "### Conclusion  \n",
    "Deep learning became popular due to advancements in **hardware**, **data availability**, and **algorithm improvements**. These factors have made deep learning essential in industries like healthcare, finance, and autonomous driving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95e887-8fa3-46fb-84bb-5d6883f131bb",
   "metadata": {},
   "source": [
    "# Neural Networks for Binary Classification\n",
    "\n",
    "## 1. **Perceptron**\n",
    "\n",
    "A **Perceptron** is the simplest form of a neural network. It is a linear classifier that can classify data points into two classes. The output is calculated by summing the weighted inputs and applying a threshold. If the sum exceeds a threshold, the output is 1, else 0.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Single-layer Neural Network**\n",
    "\n",
    "A **Single-layer Neural Network** consists of an input layer, a hidden layer, and an output layer. The hidden layer applies an activation function to the weighted sum of the inputs, and the output layer processes the result to give a final output.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Two-layer Neural Network**\n",
    "\n",
    "A **Two-layer Neural Network** has two hidden layers in between the input and output layers. The output is passed through the first hidden layer, then to the second hidden layer, and finally to the output layer. This structure allows the network to learn more complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "### 1. **Sigmoid Activation Function**\n",
    "\n",
    "The **sigmoid** function maps any input to a value between 0 and 1, making it useful for binary classification. It outputs values closer to 0 or 1, depending on the input.\n",
    "\n",
    "#### Advantages:\n",
    "- Simple and easy to understand.\n",
    "- Outputs probabilities for binary classification.\n",
    "- Useful for binary classification problems.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Prone to the **vanishing gradient problem** for large positive or negative inputs.\n",
    "- Can slow down training for deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Tanh Activation Function**\n",
    "\n",
    "The **tanh** function is similar to sigmoid but maps values to between -1 and 1. It is often used when negative outputs are needed.\n",
    "\n",
    "#### Advantages:\n",
    "- **Zero-centered**, helping with faster convergence.\n",
    "- Helps avoid the issue of saturation around zero compared to sigmoid.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Also prone to the **vanishing gradient problem**.\n",
    "- Can slow down the learning process for deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ReLU (Rectified Linear Unit) Activation Function**\n",
    "\n",
    "The **ReLU** function replaces negative values with 0 and keeps positive values unchanged. Itâ€™s computationally efficient and widely used in deep networks.\n",
    "\n",
    "#### Advantages:\n",
    "- Solves the vanishing gradient problem.\n",
    "- Fast and efficient to compute.\n",
    "- Works well for most deep learning tasks.\n",
    "\n",
    "#### Disadvantages:\n",
    "- **Dying ReLU problem**: Neurons can get stuck and stop learning if they only output zeros.\n",
    "- Can produce sparse activations, which can be inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Leaky ReLU Activation Function**\n",
    "\n",
    "**Leaky ReLU** allows small negative values instead of setting them to zero. This can help to prevent the \"dying ReLU\" problem, where neurons get stuck during training.\n",
    "\n",
    "#### Advantages:\n",
    "- Solves the **dying ReLU problem**.\n",
    "- Allows negative values, which helps the network learn better.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Can still suffer from **sparse activations**.\n",
    "- Introduces a small constant, which may not always help with training.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Parametric ReLU (PReLU)**\n",
    "\n",
    "**PReLU** is similar to Leaky ReLU, but instead of using a fixed small constant, it learns the value of the negative slope during training.\n",
    "\n",
    "#### Advantages:\n",
    "- Learns the negative slope, allowing the network to adapt during training.\n",
    "- Prevents the **dying ReLU problem** more effectively.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Adds extra parameters to the model, increasing complexity.\n",
    "- Can lead to overfitting if not regularized properly.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **ELU (Exponential Linear Unit) Activation Function**\n",
    "\n",
    "The **ELU** function helps to speed up learning by allowing negative values in the output. It can reduce bias shifts during training and improve overall performance.\n",
    "\n",
    "#### Advantages:\n",
    "- Helps with faster convergence compared to ReLU.\n",
    "- Reduces the **dying ReLU problem**.\n",
    "- Can produce outputs with both positive and negative values, which helps with training.\n",
    "\n",
    "#### Disadvantages:\n",
    "- More computationally expensive than ReLU.\n",
    "- The exponential part can make the function more sensitive to initialization and hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Softmax Activation Function**\n",
    "\n",
    "The **Softmax** function is typically used in multi-class classification tasks. It converts the raw output values into probabilities that sum up to 1, making it useful for predicting class probabilities.\n",
    "\n",
    "#### Advantages:\n",
    "- Converts raw scores into probabilities, making it useful for multi-class classification.\n",
    "- Outputs are easy to interpret as probabilities.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Not suitable for binary classification problems.\n",
    "- Can be computationally expensive when dealing with many classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent and Vanishing Gradient Problem\n",
    "\n",
    "### Chain of Derivatives:\n",
    "\n",
    "During backpropagation, we calculate how much the loss changes with respect to each weight using the **chain rule**. This helps in updating the weights efficiently during training.\n",
    "\n",
    "### Vanishing Gradient Problem:\n",
    "\n",
    "When the gradients become very small, especially with activation functions like sigmoid and tanh, the weights are updated minimally, which can slow or halt the learning process. This is known as the **vanishing gradient problem** and can make training deep networks difficult.\n",
    "\n",
    "---\n",
    "\n",
    "This markdown explains neural networks (perceptron, single-layer, two-layer) and activation functions (sigmoid, tanh, ReLU, Leaky ReLU, PReLU, ELU, Softmax) in simpler terms, with clear advantages and disadvantages for each activation function, making it more accessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7ba1e-0d66-45b0-a627-8bbbdd8ad04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
